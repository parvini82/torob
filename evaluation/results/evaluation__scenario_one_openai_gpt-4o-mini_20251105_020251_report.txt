================================================================================
SCENARIO EVALUATION REPORT
================================================================================

EVALUATION METADATA
----------------------------------------
Scenario: scenario_one
Model Name: scenario_one_openai_gpt-4o-mini
Prediction File: /home/peyma/projects/torob/evaluation/scenario_predictions/predictions_scenario_one_openai_gpt-4o-mini_Ground_Truth_first10.json
Total Samples: 10
Timestamp: 2025-11-05T02:02:51.360310

MODEL EXECUTION PERFORMANCE
----------------------------------------
Successful Predictions: 10
Failed Predictions:     0
Success Rate:           100.0%
Avg Time/Product:       51.325s

EXACT MATCHING (MAVE-style)
----------------------------------------
Precision (Micro)	0.0402
Recall (Micro)		0.1014
F1 (Micro)		0.0576
Precision (Macro)	0.0237
Recall (Macro)		0.0306
F1 (Macro)		0.0265
Exact Match Rate	0.0000

SIMILARITY METRICS
----------------------------------------
Jaccard			0.0413
Dice			0.0780
Semantic Match Rate	0.8239
Weighted Semantic Rate	0.8239

PARTIAL EVALUATION
----------------------------------------
Partial Precision	0.0374
Partial Recall		0.0826
Partial F1		0.0515

LENIENT EVALUATION
----------------------------------------
Lenient Precision	0.0777
Lenient Recall		0.2227
Lenient F1		0.1146

ADDITIONAL
----------------------------------------
ROUGE-1			0.2066